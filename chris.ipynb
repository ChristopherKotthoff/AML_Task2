{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a8b9ad6e-caa2-4909-bd84-3eee9eedbe6d",
   "metadata": {},
   "source": [
    "ideas\n",
    "todo: use random forest, boosted trees, adaboost, gradient boosted trees\n",
    "todo: look at mean heartbeats\n",
    "todo: 5 class representative beats and divergence from the as features\n",
    "todo: check if crop, inv do anything useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b102df59-1b94-4d66-a328-5b5556f7c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import pipeline\n",
    "from chris import ldData, mlpClassification, makeTrainValSet, balanceStupid, NO_DISPLAY_savePred, mlpConvolution, medmeanFeatures, biosppyECG\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from heinrich import inv, crop, ecgExtract, rfClassification\n",
    "from anova import anova"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06165cdf-4fda-4ed9-8fe8-36806882207d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61dedf29-777d-42e8-af36-7e3005d5c17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] Saved state found: ./cache/ldData()_crop(300)_inv(0.6)_biosppyECG()_medmeanFeatures(True,True,1)_balanceStupid()_makeTrainValSet(0.1), starting from function: mlpConvolution\n",
      "[Pipeline] executing: mlpConvolution(200,True,True)\n",
      "using device cpu\n",
      "\tEpoch 0 | Batch 0 | Loss   0.21\n",
      "\tEpoch 0 | Batch 40 | Loss   0.47\n",
      "\tEpoch 0 | Batch 80 | Loss   0.58\n",
      "\tEpoch 0 | Batch 120 | Loss   0.42\n",
      "\tEpoch 0 | Batch 160 | Loss   0.42\n",
      "\tEpoch 0 | Batch 200 | Loss   0.15\n",
      "\tEpoch 0 | Batch 240 | Loss   0.39\n",
      "\tEpoch 0 | Batch 280 | Loss   0.52\n",
      "\tEpoch 0 | Batch 320 | Loss   0.51\n",
      "--- TRAINING Epoch 0 | Loss   0.42\n",
      "--- VALIDATION Epoch 0 | Loss 0.47801\n",
      "\n",
      "\tEpoch 1 | Batch 0 | Loss   0.50\n",
      "\tEpoch 1 | Batch 40 | Loss   0.50\n",
      "\tEpoch 1 | Batch 80 | Loss   0.49\n",
      "\tEpoch 1 | Batch 120 | Loss   0.49\n",
      "\tEpoch 1 | Batch 160 | Loss   0.49\n",
      "\tEpoch 1 | Batch 200 | Loss   0.53\n",
      "\tEpoch 1 | Batch 240 | Loss   0.52\n",
      "\tEpoch 1 | Batch 280 | Loss   0.52\n",
      "\tEpoch 1 | Batch 320 | Loss   0.52\n",
      "--- TRAINING Epoch 1 | Loss   0.49\n",
      "--- VALIDATION Epoch 1 | Loss 0.51425\n",
      "\n",
      "\tEpoch 2 | Batch 0 | Loss   0.52\n",
      "\tEpoch 2 | Batch 40 | Loss   0.51\n",
      "\tEpoch 2 | Batch 80 | Loss   0.51\n",
      "\tEpoch 2 | Batch 120 | Loss   0.51\n",
      "\tEpoch 2 | Batch 160 | Loss   0.50\n",
      "\tEpoch 2 | Batch 200 | Loss   0.50\n",
      "\tEpoch 2 | Batch 240 | Loss   0.50\n",
      "\tEpoch 2 | Batch 280 | Loss   0.50\n",
      "\tEpoch 2 | Batch 320 | Loss   0.51\n",
      "--- TRAINING Epoch 2 | Loss   0.51\n",
      "--- VALIDATION Epoch 2 | Loss 0.50395\n",
      "\n",
      "\tEpoch 3 | Batch 0 | Loss   0.50\n",
      "\tEpoch 3 | Batch 40 | Loss   0.50\n",
      "\tEpoch 3 | Batch 80 | Loss   0.49\n",
      "\tEpoch 3 | Batch 120 | Loss   0.50\n",
      "\tEpoch 3 | Batch 160 | Loss   0.49\n",
      "\tEpoch 3 | Batch 200 | Loss   0.49\n",
      "\tEpoch 3 | Batch 240 | Loss   0.49\n",
      "\tEpoch 3 | Batch 280 | Loss   0.49\n",
      "\tEpoch 3 | Batch 320 | Loss   0.49\n",
      "--- TRAINING Epoch 3 | Loss   0.50\n",
      "--- VALIDATION Epoch 3 | Loss 0.49549\n",
      "\n",
      "\tEpoch 4 | Batch 0 | Loss   0.50\n",
      "\tEpoch 4 | Batch 40 | Loss   0.49\n",
      "\tEpoch 4 | Batch 80 | Loss   0.50\n",
      "\tEpoch 4 | Batch 120 | Loss   0.50\n",
      "\tEpoch 4 | Batch 160 | Loss   0.49\n",
      "\tEpoch 4 | Batch 200 | Loss   0.48\n",
      "\tEpoch 4 | Batch 240 | Loss   0.48\n",
      "\tEpoch 4 | Batch 280 | Loss   0.49\n",
      "\tEpoch 4 | Batch 320 | Loss   0.49\n",
      "--- TRAINING Epoch 4 | Loss   0.49\n",
      "--- VALIDATION Epoch 4 | Loss 0.48240\n",
      "\n",
      "\tEpoch 5 | Batch 0 | Loss   0.48\n",
      "\tEpoch 5 | Batch 40 | Loss   0.48\n",
      "\tEpoch 5 | Batch 80 | Loss   0.49\n",
      "\tEpoch 5 | Batch 120 | Loss   0.48\n",
      "\tEpoch 5 | Batch 160 | Loss   0.47\n",
      "\tEpoch 5 | Batch 200 | Loss   0.49\n",
      "\tEpoch 5 | Batch 240 | Loss   0.49\n",
      "\tEpoch 5 | Batch 280 | Loss   0.48\n",
      "\tEpoch 5 | Batch 320 | Loss   0.46\n",
      "--- TRAINING Epoch 5 | Loss   0.47\n",
      "--- VALIDATION Epoch 5 | Loss 0.46992\n",
      "\n",
      "\tEpoch 6 | Batch 0 | Loss   0.48\n",
      "\tEpoch 6 | Batch 40 | Loss   0.46\n",
      "\tEpoch 6 | Batch 80 | Loss   0.45\n",
      "\tEpoch 6 | Batch 120 | Loss   0.48\n",
      "\tEpoch 6 | Batch 160 | Loss   0.46\n",
      "\tEpoch 6 | Batch 200 | Loss   0.45\n",
      "\tEpoch 6 | Batch 240 | Loss   0.45\n",
      "\tEpoch 6 | Batch 280 | Loss   0.46\n",
      "\tEpoch 6 | Batch 320 | Loss   0.44\n",
      "--- TRAINING Epoch 6 | Loss   0.45\n",
      "--- VALIDATION Epoch 6 | Loss 0.44016\n",
      "\n",
      "\tEpoch 7 | Batch 0 | Loss   0.46\n",
      "\tEpoch 7 | Batch 40 | Loss   0.44\n",
      "\tEpoch 7 | Batch 80 | Loss   0.45\n",
      "\tEpoch 7 | Batch 120 | Loss   0.41\n",
      "\tEpoch 7 | Batch 160 | Loss   0.42\n",
      "\tEpoch 7 | Batch 200 | Loss   0.40\n",
      "\tEpoch 7 | Batch 240 | Loss   0.37\n",
      "\tEpoch 7 | Batch 280 | Loss   0.39\n",
      "\tEpoch 7 | Batch 320 | Loss   0.34\n",
      "--- TRAINING Epoch 7 | Loss   0.41\n",
      "--- VALIDATION Epoch 7 | Loss 0.37579\n",
      "\n",
      "\tEpoch 8 | Batch 0 | Loss   0.40\n",
      "\tEpoch 8 | Batch 40 | Loss   0.30\n",
      "\tEpoch 8 | Batch 80 | Loss   0.25\n",
      "\tEpoch 8 | Batch 120 | Loss   0.64\n",
      "\tEpoch 8 | Batch 160 | Loss   1.05\n",
      "\tEpoch 8 | Batch 200 | Loss   0.52\n",
      "\tEpoch 8 | Batch 240 | Loss   0.44\n",
      "\tEpoch 8 | Batch 280 | Loss   0.48\n",
      "\tEpoch 8 | Batch 320 | Loss   0.50\n",
      "--- TRAINING Epoch 8 | Loss   0.42\n",
      "--- VALIDATION Epoch 8 | Loss 0.46319\n",
      "\n",
      "\tEpoch 9 | Batch 0 | Loss   0.47\n",
      "\tEpoch 9 | Batch 40 | Loss   0.52\n",
      "\tEpoch 9 | Batch 80 | Loss   0.49\n",
      "\tEpoch 9 | Batch 120 | Loss   0.46\n",
      "\tEpoch 9 | Batch 160 | Loss   0.50\n",
      "\tEpoch 9 | Batch 200 | Loss   0.40\n",
      "\tEpoch 9 | Batch 240 | Loss   0.47\n",
      "\tEpoch 9 | Batch 280 | Loss   0.45\n",
      "\tEpoch 9 | Batch 320 | Loss   0.46\n",
      "--- TRAINING Epoch 9 | Loss   0.46\n",
      "--- VALIDATION Epoch 9 | Loss 0.46239\n",
      "\n",
      "\tEpoch 10 | Batch 0 | Loss   0.47\n",
      "\tEpoch 10 | Batch 40 | Loss   0.49\n",
      "\tEpoch 10 | Batch 80 | Loss   0.44\n",
      "\tEpoch 10 | Batch 120 | Loss   0.46\n",
      "\tEpoch 10 | Batch 160 | Loss   0.45\n",
      "\tEpoch 10 | Batch 200 | Loss   0.43\n",
      "\tEpoch 10 | Batch 240 | Loss   0.47\n",
      "\tEpoch 10 | Batch 280 | Loss   0.44\n",
      "\tEpoch 10 | Batch 320 | Loss   0.45\n",
      "--- TRAINING Epoch 10 | Loss   0.46\n",
      "--- VALIDATION Epoch 10 | Loss 0.46097\n",
      "\n",
      "\tEpoch 11 | Batch 0 | Loss   0.48\n",
      "\tEpoch 11 | Batch 40 | Loss   0.49\n",
      "\tEpoch 11 | Batch 80 | Loss   0.46\n",
      "\tEpoch 11 | Batch 120 | Loss   0.45\n",
      "\tEpoch 11 | Batch 160 | Loss   0.45\n",
      "\tEpoch 11 | Batch 200 | Loss   0.47\n",
      "\tEpoch 11 | Batch 240 | Loss   0.37\n",
      "\tEpoch 11 | Batch 280 | Loss   0.48\n",
      "\tEpoch 11 | Batch 320 | Loss   0.45\n",
      "--- TRAINING Epoch 11 | Loss   0.46\n",
      "--- VALIDATION Epoch 11 | Loss 0.45608\n",
      "\n",
      "\tEpoch 12 | Batch 0 | Loss   0.47\n",
      "\tEpoch 12 | Batch 40 | Loss   0.48\n",
      "\tEpoch 12 | Batch 80 | Loss   0.44\n",
      "\tEpoch 12 | Batch 120 | Loss   0.47\n",
      "\tEpoch 12 | Batch 160 | Loss   0.45\n",
      "\tEpoch 12 | Batch 200 | Loss   0.44\n",
      "\tEpoch 12 | Batch 240 | Loss   0.48\n",
      "\tEpoch 12 | Batch 280 | Loss   0.46\n",
      "\tEpoch 12 | Batch 320 | Loss   0.39\n",
      "--- TRAINING Epoch 12 | Loss   0.46\n",
      "--- VALIDATION Epoch 12 | Loss 0.45497\n",
      "\n",
      "\tEpoch 13 | Batch 0 | Loss   0.53\n",
      "\tEpoch 13 | Batch 40 | Loss   0.41\n",
      "\tEpoch 13 | Batch 80 | Loss   0.41\n",
      "\tEpoch 13 | Batch 120 | Loss   0.46\n",
      "\tEpoch 13 | Batch 160 | Loss   0.42\n",
      "\tEpoch 13 | Batch 200 | Loss   0.48\n",
      "\tEpoch 13 | Batch 240 | Loss   0.50\n",
      "\tEpoch 13 | Batch 280 | Loss   0.41\n",
      "\tEpoch 13 | Batch 320 | Loss   0.43\n",
      "--- TRAINING Epoch 13 | Loss   0.45\n",
      "--- VALIDATION Epoch 13 | Loss 0.45185\n",
      "\n",
      "\tEpoch 14 | Batch 0 | Loss   0.47\n",
      "\tEpoch 14 | Batch 40 | Loss   0.43\n",
      "\tEpoch 14 | Batch 80 | Loss   0.46\n",
      "\tEpoch 14 | Batch 120 | Loss   0.42\n",
      "\tEpoch 14 | Batch 160 | Loss   0.41\n",
      "\tEpoch 14 | Batch 200 | Loss   0.46\n",
      "\tEpoch 14 | Batch 240 | Loss   0.44\n",
      "\tEpoch 14 | Batch 280 | Loss   0.48\n",
      "\tEpoch 14 | Batch 320 | Loss   0.49\n",
      "--- TRAINING Epoch 14 | Loss   0.45\n",
      "--- VALIDATION Epoch 14 | Loss 0.44610\n",
      "\n",
      "\tEpoch 15 | Batch 0 | Loss   0.47\n",
      "\tEpoch 15 | Batch 40 | Loss   0.52\n",
      "\tEpoch 15 | Batch 80 | Loss   0.40\n",
      "\tEpoch 15 | Batch 120 | Loss   0.48\n",
      "\tEpoch 15 | Batch 160 | Loss   0.44\n",
      "\tEpoch 15 | Batch 200 | Loss   0.40\n",
      "\tEpoch 15 | Batch 240 | Loss   0.49\n",
      "\tEpoch 15 | Batch 280 | Loss   0.44\n",
      "\tEpoch 15 | Batch 320 | Loss   0.40\n",
      "--- TRAINING Epoch 15 | Loss   0.45\n",
      "--- VALIDATION Epoch 15 | Loss 0.43904\n",
      "\n",
      "\tEpoch 16 | Batch 0 | Loss   0.43\n",
      "\tEpoch 16 | Batch 40 | Loss   0.44\n",
      "\tEpoch 16 | Batch 80 | Loss   0.46\n",
      "\tEpoch 16 | Batch 120 | Loss   0.42\n",
      "\tEpoch 16 | Batch 160 | Loss   0.45\n",
      "\tEpoch 16 | Batch 200 | Loss   0.50\n",
      "\tEpoch 16 | Batch 240 | Loss   0.47\n",
      "\tEpoch 16 | Batch 280 | Loss   0.50\n",
      "\tEpoch 16 | Batch 320 | Loss   0.48\n",
      "--- TRAINING Epoch 16 | Loss   0.44\n",
      "--- VALIDATION Epoch 16 | Loss 0.43368\n",
      "\n",
      "\tEpoch 17 | Batch 0 | Loss   0.43\n",
      "\tEpoch 17 | Batch 40 | Loss   0.35\n",
      "\tEpoch 17 | Batch 80 | Loss   0.41\n",
      "\tEpoch 17 | Batch 120 | Loss   0.50\n",
      "\tEpoch 17 | Batch 160 | Loss   0.48\n",
      "\tEpoch 17 | Batch 200 | Loss   0.49\n",
      "\tEpoch 17 | Batch 240 | Loss   0.41\n",
      "\tEpoch 17 | Batch 280 | Loss   0.33\n",
      "\tEpoch 17 | Batch 320 | Loss   0.44\n",
      "--- TRAINING Epoch 17 | Loss   0.43\n",
      "--- VALIDATION Epoch 17 | Loss 0.41956\n",
      "\n",
      "\tEpoch 18 | Batch 0 | Loss   0.39\n",
      "\tEpoch 18 | Batch 40 | Loss   0.45\n",
      "\tEpoch 18 | Batch 80 | Loss   0.43\n",
      "\tEpoch 18 | Batch 120 | Loss   0.42\n",
      "\tEpoch 18 | Batch 160 | Loss   0.42\n",
      "\tEpoch 18 | Batch 200 | Loss   0.45\n",
      "\tEpoch 18 | Batch 240 | Loss   0.38\n",
      "\tEpoch 18 | Batch 280 | Loss   0.40\n",
      "\tEpoch 18 | Batch 320 | Loss   0.39\n",
      "--- TRAINING Epoch 18 | Loss   0.41\n",
      "--- VALIDATION Epoch 18 | Loss 0.39354\n",
      "\n",
      "\tEpoch 19 | Batch 0 | Loss   0.42\n",
      "\tEpoch 19 | Batch 40 | Loss   0.45\n",
      "\tEpoch 19 | Batch 80 | Loss   0.43\n",
      "\tEpoch 19 | Batch 120 | Loss   0.32\n",
      "\tEpoch 19 | Batch 160 | Loss   0.42\n",
      "\tEpoch 19 | Batch 200 | Loss   0.35\n",
      "\tEpoch 19 | Batch 240 | Loss   0.30\n",
      "\tEpoch 19 | Batch 280 | Loss   0.35\n",
      "\tEpoch 19 | Batch 320 | Loss   0.43\n",
      "--- TRAINING Epoch 19 | Loss   0.38\n",
      "--- VALIDATION Epoch 19 | Loss 0.33431\n",
      "\n",
      "\tEpoch 20 | Batch 0 | Loss   0.32\n",
      "\tEpoch 20 | Batch 40 | Loss   0.37\n",
      "\tEpoch 20 | Batch 80 | Loss   0.25\n",
      "\tEpoch 20 | Batch 120 | Loss   0.40\n",
      "\tEpoch 20 | Batch 160 | Loss   0.48\n",
      "\tEpoch 20 | Batch 200 | Loss   0.51\n",
      "\tEpoch 20 | Batch 240 | Loss   0.48\n",
      "\tEpoch 20 | Batch 280 | Loss   0.50\n",
      "\tEpoch 20 | Batch 320 | Loss   0.46\n",
      "--- TRAINING Epoch 20 | Loss   0.41\n",
      "--- VALIDATION Epoch 20 | Loss 0.46538\n",
      "\n",
      "\tEpoch 21 | Batch 0 | Loss   0.46\n",
      "\tEpoch 21 | Batch 40 | Loss   0.47\n",
      "\tEpoch 21 | Batch 80 | Loss   0.44\n",
      "\tEpoch 21 | Batch 120 | Loss   0.49\n",
      "\tEpoch 21 | Batch 160 | Loss   0.46\n",
      "\tEpoch 21 | Batch 200 | Loss   0.46\n",
      "\tEpoch 21 | Batch 240 | Loss   0.43\n",
      "\tEpoch 21 | Batch 280 | Loss   0.46\n",
      "\tEpoch 21 | Batch 320 | Loss   0.45\n",
      "--- TRAINING Epoch 21 | Loss   0.47\n",
      "--- VALIDATION Epoch 21 | Loss 0.45771\n",
      "\n",
      "\tEpoch 22 | Batch 0 | Loss   0.45\n",
      "\tEpoch 22 | Batch 40 | Loss   0.46\n",
      "\tEpoch 22 | Batch 80 | Loss   0.46\n",
      "\tEpoch 22 | Batch 120 | Loss   0.47\n",
      "\tEpoch 22 | Batch 160 | Loss   0.46\n",
      "\tEpoch 22 | Batch 200 | Loss   0.41\n",
      "\tEpoch 22 | Batch 240 | Loss   0.46\n",
      "\tEpoch 22 | Batch 280 | Loss   0.43\n",
      "\tEpoch 22 | Batch 320 | Loss   0.45\n",
      "--- TRAINING Epoch 22 | Loss   0.45\n",
      "--- VALIDATION Epoch 22 | Loss 0.44169\n",
      "\n",
      "\tEpoch 23 | Batch 0 | Loss   0.42\n",
      "\tEpoch 23 | Batch 40 | Loss   0.44\n",
      "\tEpoch 23 | Batch 80 | Loss   0.44\n",
      "\tEpoch 23 | Batch 120 | Loss   0.49\n",
      "\tEpoch 23 | Batch 160 | Loss   0.45\n",
      "\tEpoch 23 | Batch 200 | Loss   0.41\n",
      "\tEpoch 23 | Batch 240 | Loss   0.47\n",
      "\tEpoch 23 | Batch 280 | Loss   0.41\n",
      "\tEpoch 23 | Batch 320 | Loss   0.45\n",
      "--- TRAINING Epoch 23 | Loss   0.44\n",
      "--- VALIDATION Epoch 23 | Loss 0.42914\n",
      "\n",
      "\tEpoch 24 | Batch 0 | Loss   0.47\n",
      "\tEpoch 24 | Batch 40 | Loss   0.48\n",
      "\tEpoch 24 | Batch 80 | Loss   0.47\n",
      "\tEpoch 24 | Batch 120 | Loss   0.38\n",
      "\tEpoch 24 | Batch 160 | Loss   0.40\n",
      "\tEpoch 24 | Batch 200 | Loss   0.40\n",
      "\tEpoch 24 | Batch 240 | Loss   0.42\n",
      "\tEpoch 24 | Batch 280 | Loss   0.39\n",
      "\tEpoch 24 | Batch 320 | Loss   0.39\n",
      "--- TRAINING Epoch 24 | Loss   0.42\n",
      "--- VALIDATION Epoch 24 | Loss 0.40125\n",
      "\n",
      "\tEpoch 25 | Batch 0 | Loss   0.42\n",
      "\tEpoch 25 | Batch 40 | Loss   0.42\n",
      "\tEpoch 25 | Batch 80 | Loss   0.41\n",
      "\tEpoch 25 | Batch 120 | Loss   0.40\n",
      "\tEpoch 25 | Batch 160 | Loss   0.46\n",
      "\tEpoch 25 | Batch 200 | Loss   0.39\n",
      "\tEpoch 25 | Batch 240 | Loss   0.44\n",
      "\tEpoch 25 | Batch 280 | Loss   0.40\n",
      "\tEpoch 25 | Batch 320 | Loss   0.37\n",
      "--- TRAINING Epoch 25 | Loss   0.39\n",
      "--- VALIDATION Epoch 25 | Loss 0.35919\n",
      "\n",
      "\tEpoch 26 | Batch 0 | Loss   0.34\n",
      "\tEpoch 26 | Batch 40 | Loss   0.36\n",
      "\tEpoch 26 | Batch 80 | Loss   0.30\n",
      "\tEpoch 26 | Batch 120 | Loss   0.38\n",
      "\tEpoch 26 | Batch 160 | Loss   0.34\n",
      "\tEpoch 26 | Batch 200 | Loss   0.25\n",
      "\tEpoch 26 | Batch 240 | Loss   0.08\n",
      "\tEpoch 26 | Batch 280 | Loss   0.50\n",
      "\tEpoch 26 | Batch 320 | Loss   0.50\n",
      "--- TRAINING Epoch 26 | Loss   0.66\n",
      "--- VALIDATION Epoch 26 | Loss 0.55257\n",
      "\n",
      "\tEpoch 27 | Batch 0 | Loss   0.54\n",
      "\tEpoch 27 | Batch 40 | Loss   0.51\n",
      "\tEpoch 27 | Batch 80 | Loss   0.46\n",
      "\tEpoch 27 | Batch 120 | Loss   0.47\n",
      "\tEpoch 27 | Batch 160 | Loss   0.52\n",
      "\tEpoch 27 | Batch 200 | Loss   0.48\n",
      "\tEpoch 27 | Batch 240 | Loss   0.35\n",
      "\tEpoch 27 | Batch 280 | Loss   0.40\n",
      "\tEpoch 27 | Batch 320 | Loss   0.51\n",
      "--- TRAINING Epoch 27 | Loss   0.57\n",
      "--- VALIDATION Epoch 27 | Loss 0.47666\n",
      "\n",
      "\tEpoch 28 | Batch 0 | Loss   0.41\n",
      "\tEpoch 28 | Batch 40 | Loss   0.99\n",
      "\tEpoch 28 | Batch 80 | Loss   0.62\n",
      "\tEpoch 28 | Batch 120 | Loss   0.58\n",
      "\tEpoch 28 | Batch 160 | Loss   0.85\n",
      "\tEpoch 28 | Batch 200 | Loss   4.18\n",
      "\tEpoch 28 | Batch 240 | Loss   0.63\n",
      "\tEpoch 28 | Batch 280 | Loss   1.00\n",
      "\tEpoch 28 | Batch 320 | Loss   0.56\n",
      "--- TRAINING Epoch 28 | Loss   0.37\n",
      "--- VALIDATION Epoch 28 | Loss 0.18698\n",
      "\n",
      "\tEpoch 29 | Batch 0 | Loss   0.59\n",
      "\tEpoch 29 | Batch 40 | Loss   0.55\n",
      "\tEpoch 29 | Batch 80 | Loss   0.48\n",
      "\tEpoch 29 | Batch 120 | Loss   1.84\n",
      "\tEpoch 29 | Batch 160 | Loss   0.56\n",
      "\tEpoch 29 | Batch 200 | Loss   0.46\n",
      "\tEpoch 29 | Batch 240 | Loss   0.60\n",
      "\tEpoch 29 | Batch 280 | Loss   0.59\n",
      "\tEpoch 29 | Batch 320 | Loss   0.55\n",
      "--- TRAINING Epoch 29 | Loss   0.62\n",
      "--- VALIDATION Epoch 29 | Loss 0.48490\n",
      "\n",
      "\tEpoch 30 | Batch 0 | Loss   0.51\n",
      "\tEpoch 30 | Batch 40 | Loss   0.55\n",
      "\tEpoch 30 | Batch 80 | Loss   0.60\n",
      "\tEpoch 30 | Batch 120 | Loss   0.38\n",
      "\tEpoch 30 | Batch 160 | Loss   0.10\n",
      "\tEpoch 30 | Batch 200 | Loss   0.58\n",
      "\tEpoch 30 | Batch 240 | Loss   0.53\n",
      "\tEpoch 30 | Batch 280 | Loss   0.50\n",
      "\tEpoch 30 | Batch 320 | Loss   0.54\n",
      "--- TRAINING Epoch 30 | Loss   0.52\n",
      "--- VALIDATION Epoch 30 | Loss 0.51214\n",
      "\n",
      "\tEpoch 31 | Batch 0 | Loss   0.55\n",
      "\tEpoch 31 | Batch 40 | Loss   0.57\n",
      "\tEpoch 31 | Batch 80 | Loss   0.52\n",
      "\tEpoch 31 | Batch 120 | Loss   0.84\n",
      "\tEpoch 31 | Batch 160 | Loss   0.50\n",
      "\tEpoch 31 | Batch 200 | Loss   0.48\n",
      "\tEpoch 31 | Batch 240 | Loss   0.56\n",
      "\tEpoch 31 | Batch 280 | Loss   8.18\n",
      "\tEpoch 31 | Batch 320 | Loss   0.48\n",
      "--- TRAINING Epoch 31 | Loss   0.55\n",
      "--- VALIDATION Epoch 31 | Loss 0.50292\n",
      "\n",
      "\tEpoch 32 | Batch 0 | Loss   0.53\n",
      "\tEpoch 32 | Batch 40 | Loss   0.49\n",
      "\tEpoch 32 | Batch 80 | Loss   0.51\n",
      "\tEpoch 32 | Batch 120 | Loss   0.52\n",
      "\tEpoch 32 | Batch 160 | Loss   0.48\n",
      "\tEpoch 32 | Batch 200 | Loss   0.47\n",
      "\tEpoch 32 | Batch 240 | Loss   0.48\n",
      "\tEpoch 32 | Batch 280 | Loss   0.58\n",
      "\tEpoch 32 | Batch 320 | Loss   0.55\n",
      "--- TRAINING Epoch 32 | Loss   0.55\n",
      "--- VALIDATION Epoch 32 | Loss 0.51926\n",
      "\n",
      "\tEpoch 33 | Batch 0 | Loss   0.50\n",
      "\tEpoch 33 | Batch 40 | Loss   0.50\n",
      "\tEpoch 33 | Batch 80 | Loss   0.52\n",
      "\tEpoch 33 | Batch 120 | Loss   0.38\n",
      "\tEpoch 33 | Batch 160 | Loss   0.48\n",
      "\tEpoch 33 | Batch 200 | Loss   0.44\n",
      "\tEpoch 33 | Batch 240 | Loss   0.55\n",
      "\tEpoch 33 | Batch 280 | Loss   0.43\n",
      "\tEpoch 33 | Batch 320 | Loss   0.45\n",
      "--- TRAINING Epoch 33 | Loss   0.25\n",
      "--- VALIDATION Epoch 33 | Loss 0.45019\n",
      "\n",
      "\tEpoch 34 | Batch 0 | Loss   0.49\n",
      "\tEpoch 34 | Batch 40 | Loss   0.50\n",
      "\tEpoch 34 | Batch 80 | Loss   0.49\n",
      "\tEpoch 34 | Batch 120 | Loss   0.44\n",
      "\tEpoch 34 | Batch 160 | Loss   0.51\n",
      "\tEpoch 34 | Batch 200 | Loss   0.41\n",
      "\tEpoch 34 | Batch 240 | Loss   0.45\n",
      "\tEpoch 34 | Batch 280 | Loss   0.48\n",
      "\tEpoch 34 | Batch 320 | Loss   0.44\n",
      "--- TRAINING Epoch 34 | Loss   0.44\n",
      "--- VALIDATION Epoch 34 | Loss 0.49339\n",
      "\n",
      "\tEpoch 35 | Batch 0 | Loss   0.51\n",
      "\tEpoch 35 | Batch 40 | Loss   0.46\n",
      "\tEpoch 35 | Batch 80 | Loss   0.46\n",
      "\tEpoch 35 | Batch 120 | Loss   0.43\n",
      "\tEpoch 35 | Batch 160 | Loss   0.38\n",
      "\tEpoch 35 | Batch 200 | Loss   0.53\n",
      "\tEpoch 35 | Batch 240 | Loss   0.46\n",
      "\tEpoch 35 | Batch 280 | Loss   0.75\n",
      "\tEpoch 35 | Batch 320 | Loss   0.40\n",
      "--- TRAINING Epoch 35 | Loss   0.45\n",
      "--- VALIDATION Epoch 35 | Loss -0.34945\n",
      "\n",
      "\tEpoch 36 | Batch 0 | Loss   0.45\n",
      "\tEpoch 36 | Batch 40 | Loss   0.42\n",
      "\tEpoch 36 | Batch 80 | Loss   0.46\n",
      "\tEpoch 36 | Batch 120 | Loss   0.44\n",
      "\tEpoch 36 | Batch 160 | Loss   0.50\n",
      "\tEpoch 36 | Batch 200 | Loss   0.46\n",
      "\tEpoch 36 | Batch 240 | Loss   0.41\n",
      "\tEpoch 36 | Batch 280 | Loss   0.42\n",
      "\tEpoch 36 | Batch 320 | Loss   0.43\n",
      "--- TRAINING Epoch 36 | Loss   0.46\n",
      "--- VALIDATION Epoch 36 | Loss 0.41594\n",
      "\n",
      "\tEpoch 37 | Batch 0 | Loss   0.52\n",
      "\tEpoch 37 | Batch 40 | Loss   0.51\n",
      "\tEpoch 37 | Batch 80 | Loss   0.49\n",
      "\tEpoch 37 | Batch 120 | Loss   0.48\n",
      "\tEpoch 37 | Batch 160 | Loss   0.44\n",
      "\tEpoch 37 | Batch 200 | Loss   0.50\n",
      "\tEpoch 37 | Batch 240 | Loss   0.54\n",
      "\tEpoch 37 | Batch 280 | Loss   0.47\n",
      "\tEpoch 37 | Batch 320 | Loss   0.41\n",
      "--- TRAINING Epoch 37 | Loss   0.46\n",
      "--- VALIDATION Epoch 37 | Loss 0.47506\n",
      "\n",
      "\tEpoch 38 | Batch 0 | Loss   0.43\n",
      "\tEpoch 38 | Batch 40 | Loss   0.44\n",
      "\tEpoch 38 | Batch 80 | Loss   0.44\n",
      "\tEpoch 38 | Batch 120 | Loss   0.44\n",
      "\tEpoch 38 | Batch 160 | Loss   0.48\n",
      "\tEpoch 38 | Batch 200 | Loss   0.44\n",
      "\tEpoch 38 | Batch 240 | Loss   0.49\n",
      "\tEpoch 38 | Batch 280 | Loss   0.48\n",
      "\tEpoch 38 | Batch 320 | Loss   0.40\n",
      "--- TRAINING Epoch 38 | Loss   0.46\n",
      "--- VALIDATION Epoch 38 | Loss 0.46594\n",
      "\n",
      "\tEpoch 39 | Batch 0 | Loss   0.45\n",
      "\tEpoch 39 | Batch 40 | Loss   0.66\n",
      "\tEpoch 39 | Batch 80 | Loss   0.45\n",
      "\tEpoch 39 | Batch 120 | Loss   0.44\n",
      "\tEpoch 39 | Batch 160 | Loss   0.48\n",
      "\tEpoch 39 | Batch 200 | Loss   0.44\n",
      "\tEpoch 39 | Batch 240 | Loss   0.41\n",
      "\tEpoch 39 | Batch 280 | Loss   0.51\n",
      "\tEpoch 39 | Batch 320 | Loss   0.43\n",
      "--- TRAINING Epoch 39 | Loss   0.47\n",
      "--- VALIDATION Epoch 39 | Loss 0.48233\n",
      "\n",
      "\tEpoch 40 | Batch 0 | Loss   0.50\n",
      "\tEpoch 40 | Batch 40 | Loss   0.53\n",
      "\tEpoch 40 | Batch 80 | Loss   0.43\n",
      "\tEpoch 40 | Batch 120 | Loss   0.48\n",
      "\tEpoch 40 | Batch 160 | Loss   0.45\n",
      "\tEpoch 40 | Batch 200 | Loss   0.47\n",
      "\tEpoch 40 | Batch 240 | Loss   0.50\n",
      "\tEpoch 40 | Batch 280 | Loss   0.53\n",
      "\tEpoch 40 | Batch 320 | Loss   0.49\n",
      "--- TRAINING Epoch 40 | Loss   0.46\n",
      "--- VALIDATION Epoch 40 | Loss 0.50740\n",
      "\n",
      "\tEpoch 41 | Batch 0 | Loss   0.41\n",
      "\tEpoch 41 | Batch 40 | Loss   0.56\n",
      "\tEpoch 41 | Batch 80 | Loss   0.46\n",
      "\tEpoch 41 | Batch 120 | Loss   0.61\n",
      "\tEpoch 41 | Batch 160 | Loss   0.47\n",
      "\tEpoch 41 | Batch 200 | Loss   0.45\n",
      "\tEpoch 41 | Batch 240 | Loss   0.45\n",
      "\tEpoch 41 | Batch 280 | Loss   0.45\n",
      "\tEpoch 41 | Batch 320 | Loss   0.44\n",
      "--- TRAINING Epoch 41 | Loss   0.46\n",
      "--- VALIDATION Epoch 41 | Loss 0.47602\n",
      "\n",
      "\tEpoch 42 | Batch 0 | Loss   0.40\n",
      "\tEpoch 42 | Batch 40 | Loss   0.47\n",
      "\tEpoch 42 | Batch 80 | Loss   0.44\n",
      "\tEpoch 42 | Batch 120 | Loss   0.43\n",
      "\tEpoch 42 | Batch 160 | Loss   0.48\n",
      "\tEpoch 42 | Batch 200 | Loss   0.53\n",
      "\tEpoch 42 | Batch 240 | Loss   0.45\n",
      "\tEpoch 42 | Batch 280 | Loss   0.33\n",
      "\tEpoch 42 | Batch 320 | Loss   0.42\n",
      "--- TRAINING Epoch 42 | Loss   0.46\n",
      "--- VALIDATION Epoch 42 | Loss 0.45014\n",
      "\n",
      "\tEpoch 43 | Batch 0 | Loss   0.49\n",
      "\tEpoch 43 | Batch 40 | Loss   0.45\n",
      "\tEpoch 43 | Batch 80 | Loss   0.41\n",
      "\tEpoch 43 | Batch 120 | Loss   0.47\n",
      "\tEpoch 43 | Batch 160 | Loss   0.43\n",
      "\tEpoch 43 | Batch 200 | Loss   0.38\n",
      "\tEpoch 43 | Batch 240 | Loss   0.45\n",
      "\tEpoch 43 | Batch 280 | Loss   0.48\n",
      "\tEpoch 43 | Batch 320 | Loss   0.72\n",
      "--- TRAINING Epoch 43 | Loss   0.44\n",
      "--- VALIDATION Epoch 43 | Loss 0.53994\n",
      "\n",
      "\tEpoch 44 | Batch 0 | Loss   0.45\n",
      "\tEpoch 44 | Batch 40 | Loss   0.50\n",
      "\tEpoch 44 | Batch 80 | Loss   0.47\n",
      "\tEpoch 44 | Batch 120 | Loss   0.49\n",
      "\tEpoch 44 | Batch 160 | Loss   0.39\n",
      "\tEpoch 44 | Batch 200 | Loss   0.47\n",
      "\tEpoch 44 | Batch 240 | Loss   0.48\n",
      "\tEpoch 44 | Batch 280 | Loss   0.50\n",
      "\tEpoch 44 | Batch 320 | Loss   0.44\n",
      "--- TRAINING Epoch 44 | Loss   0.43\n",
      "--- VALIDATION Epoch 44 | Loss 0.56880\n",
      "\n",
      "\tEpoch 45 | Batch 0 | Loss   0.44\n",
      "\tEpoch 45 | Batch 40 | Loss   0.30\n",
      "\tEpoch 45 | Batch 80 | Loss   0.42\n",
      "\tEpoch 45 | Batch 120 | Loss   0.47\n",
      "\tEpoch 45 | Batch 160 | Loss   0.40\n",
      "\tEpoch 45 | Batch 200 | Loss   0.48\n",
      "\tEpoch 45 | Batch 240 | Loss   0.43\n",
      "\tEpoch 45 | Batch 280 | Loss   0.48\n",
      "\tEpoch 45 | Batch 320 | Loss   0.40\n",
      "--- TRAINING Epoch 45 | Loss   0.46\n",
      "--- VALIDATION Epoch 45 | Loss 0.48737\n",
      "\n",
      "\tEpoch 46 | Batch 0 | Loss   0.48\n",
      "\tEpoch 46 | Batch 40 | Loss   0.44\n",
      "\tEpoch 46 | Batch 80 | Loss   0.45\n",
      "\tEpoch 46 | Batch 120 | Loss   0.49\n",
      "\tEpoch 46 | Batch 160 | Loss   0.43\n",
      "\tEpoch 46 | Batch 200 | Loss   0.39\n",
      "\tEpoch 46 | Batch 240 | Loss   0.49\n",
      "\tEpoch 46 | Batch 280 | Loss   0.47\n",
      "\tEpoch 46 | Batch 320 | Loss   0.43\n",
      "--- TRAINING Epoch 46 | Loss   0.46\n",
      "--- VALIDATION Epoch 46 | Loss 0.46288\n",
      "\n",
      "\tEpoch 47 | Batch 0 | Loss   0.54\n",
      "\tEpoch 47 | Batch 40 | Loss   0.45\n",
      "\tEpoch 47 | Batch 80 | Loss   0.52\n",
      "\tEpoch 47 | Batch 120 | Loss   0.46\n",
      "\tEpoch 47 | Batch 160 | Loss   0.45\n",
      "\tEpoch 47 | Batch 200 | Loss   0.51\n",
      "\tEpoch 47 | Batch 240 | Loss   0.46\n",
      "\tEpoch 47 | Batch 280 | Loss   0.46\n",
      "\tEpoch 47 | Batch 320 | Loss   0.48\n",
      "--- TRAINING Epoch 47 | Loss   0.47\n",
      "--- VALIDATION Epoch 47 | Loss 0.47356\n",
      "\n",
      "\tEpoch 48 | Batch 0 | Loss   0.44\n",
      "\tEpoch 48 | Batch 40 | Loss   0.48\n",
      "\tEpoch 48 | Batch 80 | Loss   0.41\n",
      "\tEpoch 48 | Batch 120 | Loss   0.43\n",
      "\tEpoch 48 | Batch 160 | Loss   0.52\n",
      "\tEpoch 48 | Batch 200 | Loss   0.48\n",
      "\tEpoch 48 | Batch 240 | Loss   0.50\n",
      "\tEpoch 48 | Batch 280 | Loss   0.45\n",
      "\tEpoch 48 | Batch 320 | Loss   0.44\n",
      "--- TRAINING Epoch 48 | Loss   0.47\n",
      "--- VALIDATION Epoch 48 | Loss 0.46638\n",
      "\n",
      "\tEpoch 49 | Batch 0 | Loss   0.47\n",
      "\tEpoch 49 | Batch 40 | Loss   0.55\n",
      "\tEpoch 49 | Batch 80 | Loss   0.44\n",
      "\tEpoch 49 | Batch 120 | Loss   0.40\n",
      "\tEpoch 49 | Batch 160 | Loss   0.46\n",
      "\tEpoch 49 | Batch 200 | Loss   0.49\n",
      "\tEpoch 49 | Batch 240 | Loss   0.47\n",
      "\tEpoch 49 | Batch 280 | Loss   0.47\n",
      "\tEpoch 49 | Batch 320 | Loss   0.53\n",
      "--- TRAINING Epoch 49 | Loss   0.47\n",
      "--- VALIDATION Epoch 49 | Loss 0.49597\n",
      "\n",
      "\tEpoch 50 | Batch 0 | Loss   0.46\n",
      "\tEpoch 50 | Batch 40 | Loss   0.46\n",
      "\tEpoch 50 | Batch 80 | Loss   0.46\n",
      "\tEpoch 50 | Batch 120 | Loss   0.51\n",
      "\tEpoch 50 | Batch 160 | Loss   0.49\n",
      "\tEpoch 50 | Batch 200 | Loss   0.51\n",
      "\tEpoch 50 | Batch 240 | Loss   0.48\n",
      "\tEpoch 50 | Batch 280 | Loss   0.46\n",
      "\tEpoch 50 | Batch 320 | Loss   0.42\n",
      "--- TRAINING Epoch 50 | Loss   0.46\n",
      "--- VALIDATION Epoch 50 | Loss 0.46945\n",
      "\n",
      "\tEpoch 51 | Batch 0 | Loss   0.45\n",
      "\tEpoch 51 | Batch 40 | Loss   0.42\n",
      "\tEpoch 51 | Batch 80 | Loss   0.47\n",
      "\tEpoch 51 | Batch 120 | Loss   0.43\n",
      "\tEpoch 51 | Batch 160 | Loss   0.09\n",
      "\tEpoch 51 | Batch 200 | Loss   0.44\n",
      "\tEpoch 51 | Batch 240 | Loss   0.41\n",
      "\tEpoch 51 | Batch 280 | Loss   0.48\n",
      "\tEpoch 51 | Batch 320 | Loss   0.56\n",
      "--- TRAINING Epoch 51 | Loss   0.47\n",
      "--- VALIDATION Epoch 51 | Loss 0.47146\n",
      "\n",
      "\tEpoch 52 | Batch 0 | Loss   0.44\n",
      "\tEpoch 52 | Batch 40 | Loss   0.39\n",
      "\tEpoch 52 | Batch 80 | Loss   0.44\n",
      "\tEpoch 52 | Batch 120 | Loss   0.46\n",
      "\tEpoch 52 | Batch 160 | Loss   0.45\n",
      "\tEpoch 52 | Batch 200 | Loss   0.53\n",
      "\tEpoch 52 | Batch 240 | Loss   0.52\n",
      "\tEpoch 52 | Batch 280 | Loss   0.46\n",
      "\tEpoch 52 | Batch 320 | Loss   0.47\n",
      "--- TRAINING Epoch 52 | Loss   0.48\n",
      "--- VALIDATION Epoch 52 | Loss 0.47168\n",
      "\n",
      "\tEpoch 53 | Batch 0 | Loss   0.47\n",
      "\tEpoch 53 | Batch 40 | Loss   0.50\n",
      "\tEpoch 53 | Batch 80 | Loss   0.49\n",
      "\tEpoch 53 | Batch 120 | Loss   0.54\n",
      "\tEpoch 53 | Batch 160 | Loss   0.43\n",
      "\tEpoch 53 | Batch 200 | Loss   0.42\n",
      "\tEpoch 53 | Batch 240 | Loss   0.42\n",
      "\tEpoch 53 | Batch 280 | Loss   0.48\n",
      "\tEpoch 53 | Batch 320 | Loss   0.52\n",
      "--- TRAINING Epoch 53 | Loss   0.47\n",
      "--- VALIDATION Epoch 53 | Loss 0.47989\n",
      "\n",
      "\tEpoch 54 | Batch 0 | Loss   0.42\n",
      "\tEpoch 54 | Batch 40 | Loss   0.51\n",
      "\tEpoch 54 | Batch 80 | Loss   0.45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m hyper \u001b[38;5;241m=\u001b[39m { \n\u001b[1;32m      2\u001b[0m     \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minv_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.6\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manova_percentage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.7\u001b[39m\n\u001b[1;32m     16\u001b[0m }\n\u001b[0;32m---> 18\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[43mldData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiosppyECG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmedmeanFeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbalanceStupid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmakeTrainValSet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlpConvolution\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain losses\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot( data[ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m ], color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
      "File \u001b[0;32m~/aml22/AML_Task2/pipeline.py:109\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(functionlist, hyperparameter_dictionary, use_cached_states, save_states_to_cache)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Pipeline] executing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_information[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunction_call_string\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_current_function_information_\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m function_information[i]\n\u001b[0;32m--> 109\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_information\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhyperparameter_dictionary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_dict, Mapping):\n\u001b[1;32m    111\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Pipeline] \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    112\u001b[0m     function_information[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m did not return the data_dict. Please have a look and make sure the updated data_dict is returned.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m   )\n",
      "File \u001b[0;32m~/aml22/AML_Task2/chris.py:85\u001b[0m, in \u001b[0;36mmlpConvolution\u001b[0;34m(data_dict, mlpConvolution_epochs, mlpConvolution_useValidationSet, mlpConvolution_makePrediction, **args)\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data_dict\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mlpConvolution_useValidationSet:\n\u001b[0;32m---> 85\u001b[0m   data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m], data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m], predict_funct \u001b[38;5;241m=\u001b[39m \u001b[43mchris_krimskrams_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlpConvolution_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX_val\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_val\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m   data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_val_hat\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mpredict_funct(data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_val\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     87\u001b[0m   data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train_hat\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mpredict_funct(data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/aml22/AML_Task2/chris_krimskrams_2.py:193\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, X_train, y_train, X_val, y_val, batch_size)\u001b[0m\n\u001b[1;32m    190\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    191\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 193\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m#print(output.shape)\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m#print(y.shape)\u001b[39;00m\n\u001b[1;32m    196\u001b[0m loss \u001b[38;5;241m=\u001b[39m soft_f1_loss(y, output)\n",
      "File \u001b[0;32m/usr/local/share/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/aml22/AML_Task2/chris_krimskrams_2.py:86\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 86\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirstStage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     88\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(out)\n",
      "File \u001b[0;32m/usr/local/share/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/usr/local/share/miniconda/lib/python3.9/site-packages/torch/nn/modules/container.py:119\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/share/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/usr/local/share/miniconda/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:135\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda/lib/python3.9/site-packages/torch/nn/functional.py:2149\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2147\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyper = { \n",
    "    \n",
    "    \"inv_threshold\": 0.6, \n",
    "    \"crop_location\": 300,\n",
    "    \"mlpClassification_epochs\": 200,\n",
    "    \"mlpClassification_useValidationSet\": True,\n",
    "    \"mlpClassification_makePrediction\": False,\n",
    "    \"mlpConvolution_epochs\": 200,\n",
    "    \"mlpConvolution_useValidationSet\": True,\n",
    "    \"mlpConvolution_makePrediction\": True,\n",
    "    \"makeTrainValSet_valPercent\": 0.1,\n",
    "    \"rfClassification_depth\": 3,\n",
    "    \"rfClassification_useValidationSet\": True,\n",
    "    \"rfClassification_makePrediction\": False,\n",
    "    \"anova_percentage\": 0.7\n",
    "}\n",
    "\n",
    "data = pipeline([ ldData, crop, inv, biosppyECG, medmeanFeatures,balanceStupid, makeTrainValSet, mlpConvolution ], hyper )\n",
    "print( \"train losses\" )\n",
    "plt.plot( data[ \"train_losses\" ], color = \"blue\" )\n",
    "print( \"val losses\" )\n",
    "plt.plot( data[ \"val_losses\" ], color = \"green\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e0b0b6-3e27-469d-8740-4c68bbf5555e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.cuda' from '/usr/local/share/miniconda/lib/python3.9/site-packages/torch/cuda/__init__.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83a5eb7f-66c5-42c3-87b1-9b2b7d1a73f7",
   "metadata": {},
   "source": [
    "^ this is ok, since we resampled/balanced the train data. the two data sets do not come from the same distribution, thus we do not need to worry about overfitting at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ef70d33-6ab6-456b-9f2f-0e3d753ea78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from olin_utils import confMat\n",
    "_, misclass = confMat( data[ \"y_val_hat\" ], np.transpose( data[ \"y_val\" ])[ 0 ], visualize = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848474d-397b-47c2-b5de-e45c08ce8a68",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73eb1469-b250-450b-acaf-17adc334d24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] Saved state found: ./cache/ldData()_crop(300)_inv(0.6)_ecgExtract()_anova(0.7), starting from function: rfClassification\n",
      "[Pipeline] executing: rfClassification(3,False,True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/miniconda/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] executing: NO_DISPLAY_savePred()\n",
      "train losses\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7a50298d90>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyper = { \n",
    "    \n",
    "    \"inv_threshold\": 0.6, \n",
    "    \"crop_location\": 300,\n",
    "    \"mlpClassification_epochs\": 150,\n",
    "    \"mlpClassification_useValidationSet\": False,\n",
    "    \"mlpClassification_makePrediction\": True,\n",
    "    \"makeTrainValSet_valPercent\": 0.1,\n",
    "    \"rfClassification_depth\": 3,\n",
    "    \"rfClassification_useValidationSet\": False,\n",
    "    \"rfClassification_makePrediction\": True,\n",
    "    \"anova_percentage\": 0.7\n",
    "}\n",
    "\n",
    "data = pipeline([ ldData, crop, inv, ecgExtract, anova, rfClassification, NO_DISPLAY_savePred ], hyper )\n",
    "print( \"train losses\" )\n",
    "plt.plot( data[ \"train_losses\" ], color = \"blue\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
