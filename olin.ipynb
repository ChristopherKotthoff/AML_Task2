{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a1f297-e4d7-432d-b41b-f9433e868071",
   "metadata": {},
   "source": [
    "## Implementation of a stacked Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7747a498-f47f-4d54-ad19-248b59cf13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "def train_stacked(X_train, y_train, X_val, y_val, show_validation_perf = False):\n",
    "    # TODO: take different train/test splits to evaluate the performance development, #if show_validation_perf:\n",
    "    \n",
    "    X_train = X_train\n",
    "    y_train = y_train\n",
    "    \n",
    "    # normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_val_norm = scaler.transform(X_val)\n",
    "    \n",
    "    # create a list of classifiers to train inside the stacked model\n",
    "    estimators = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "        ('linsvr', LinearSVC(random_state=42)),\n",
    "        ('rbfsvc', SVC(kernel = 'rbf', gamma='auto', probability=True, random_state=42)),\n",
    "        ('sigmoidsvc', SVC(kernel = 'sigmoid', gamma='auto', probability=True, random_state=42)),\n",
    "        ('polysvc', SVC(kernel = 'poly', gamma='auto', probability=True, random_state=42)),\n",
    "        ('gradBoost', GradientBoostingClassifier( max_depth = 4, random_state = 42, learning_rate = 0.05, n_estimators = 500, min_samples_split = 20, max_features = 0.2 )),\n",
    "        ('mlp', MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=5000, learning_rate='adaptive', batch_size=32, random_state=42)),\n",
    "        ('uniformknn', KNeighborsClassifier(n_neighbors = 15, weights='uniform')),\n",
    "        ('distknn', KNeighborsClassifier(n_neighbors = 15, weights='distance')),\n",
    "        ('adaboost', AdaBoostClassifier(random_state=42)),\n",
    "        ('quadDiscAna', QuadraticDiscriminantAnalysis())  #Quadratic Discriminant Analysis\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # combining the previously defined classifiers into one Stacked Clf\n",
    "    clf = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        #final_estimator=LogisticRegression(), # try mlp\n",
    "        final_estimator=MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=5000, learning_rate='adaptive', batch_size=32, random_state=42),\n",
    "        verbose = 1\n",
    "    )\n",
    "\n",
    "    # fit the ensemble clf on the train data\n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "    #create a prediction function that deploys the trained stacked classifier\n",
    "    predict_funct = lambda X: clf.predict(X)\n",
    "    \n",
    "    # define a score\n",
    "    score = lambda y, y_hat : 1 - f1_score( y, y_hat, average = \"micro\" )\n",
    "    train_loss_timeseries = np.repeat([ score( y_train, clf.predict( X_train_norm ))], 2 )\n",
    "        \n",
    "    if X_val is not None:\n",
    "        val_loss_timeseries = np.repeat([ score( y_val, clf.predict( X_val_norm ))], 2 )\n",
    "        return train_loss_timeseries, val_loss_timeseries, predict_funct\n",
    "    else:\n",
    "        return train_loss_timeseries, predict_funct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e8c2e770-1105-416a-a5d7-558593332b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HÃ¼lle kopiert von chris mlpClassifier\n",
    "\n",
    "#import stackedClf_Olin # needs to contains .train()\n",
    "\n",
    "def stackedClassification(data_dict, stackedClf_useValidationSet, stackedClf_makePrediction, **args):\n",
    "    \n",
    "    assert \"X_train\" in data_dict.keys()\n",
    "    assert \"y_train\" in data_dict.keys()\n",
    "    if stackedClf_useValidationSet:\n",
    "        assert \"X_val\" in data_dict.keys()\n",
    "        assert \"y_val\" in data_dict.keys()\n",
    "    if stackedClf_makePrediction:\n",
    "        assert \"X_test\" in data_dict.keys()\n",
    "\n",
    "\n",
    "    if stackedClf_useValidationSet:\n",
    "        data_dict[\"train_losses\"], data_dict[\"val_losses\"], stacked_predict_funct = train_stacked( X_train=data_dict[\"X_train\"], y_train=data_dict[\"y_train\"], X_val=data_dict[\"X_val\"], y_val=data_dict[\"y_val\"])\n",
    "        data_dict[\"y_val_hat\"]=stacked_predict_funct(data_dict[\"X_val\"])\n",
    "        data_dict[\"y_train_hat\"]=stacked_predict_funct(data_dict[\"X_train\"])\n",
    "    else:\n",
    "        data_dict[\"train_losses\"], predict_funct = train_stacked(X_train=data_dict[\"X_train\"], y_train=data_dict[\"y_train\"])\n",
    "        data_dict[\"y_train_hat\"]=stacked_predict_funct(data_dict[\"X_train\"])\n",
    "\n",
    "    if stackedClf_makePrediction:\n",
    "        data_dict[\"y_test\"] = stacked_predict_funct(data_dict[\"X_test\"])\n",
    "\n",
    "    if stackedClf_useValidationSet:\n",
    "        data_dict[\"y_val_predicted\"] = stacked_predict_funct(data_dict[\"X_val\"])\n",
    "    \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e61772d-cf48-4844-886e-ff247dafb4ca",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "72100107-5141-4535-9447-8f8a878f0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import pipeline\n",
    "from chris import ldData, mlpClassification, makeTrainValSet, balanceStupid, NO_DISPLAY_savePred\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from heinrich import inv, crop, ecgExtract, rfClassification\n",
    "from anova import anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c26232-019c-4984-9671-3269d164d6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] Saved state found: ./cache/ldData()_crop(300)_inv(0.6)_ecgExtract()_makeTrainValSet(0.1), starting from function: stackedClassification\n",
      "[Pipeline] executing: stackedClassification(True,False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/miniconda/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/share/miniconda/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/share/miniconda/lib/python3.9/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hyper = { \n",
    "    \n",
    "    \"inv_threshold\": 0.6, \n",
    "    \"crop_location\": 300,\n",
    "    \"mlpClassification_epochs\": 200,\n",
    "    \"mlpClassification_useValidationSet\": True,\n",
    "    \"mlpClassification_makePrediction\": False,\n",
    "    \"makeTrainValSet_valPercent\": 0.1,\n",
    "    \"rfClassification_depth\": 3,\n",
    "    \"rfClassification_useValidationSet\": True,\n",
    "    \"rfClassification_makePrediction\": False,\n",
    "    \"anova_percentage\": 0.7,\n",
    "    \"stackedClf_useValidationSet\": True,\n",
    "    \"stackedClf_makePrediction\": False\n",
    "    \n",
    "}\n",
    "\n",
    "data = pipeline([ ldData, crop, inv, ecgExtract, makeTrainValSet, stackedClassification ], hyper, save_states_to_cache=True)\n",
    "print( \"train losses\" )\n",
    "data[\"train_losses\"]\n",
    "print( \"val losses\" )\n",
    "data[\"val_losses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d80f8b9-93af-4ecc-8495-ba9deed69192",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train_losses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7563877a-e04b-49c5-8db9-2c60bcce4843",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"val_losses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5065e871-2fa6-4dec-9033-7c762aa4f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from olin_utils import confMat\n",
    "_, misclass = confMat( data[ \"y_val_hat\" ], np.transpose( data[ \"y_val\" ])[ 0 ], visualize = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa86dc-fd92-4db4-a5ae-5771f6915d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e622a-2653-4db2-bf05-a14b2144c441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a4250-7e4e-427a-a9ac-a0e904d8e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# create a dictionary for the parameters of the classifiers in the ensamble\n",
    "para_dict = {}\n",
    "para_dict[\"random_state\"]=42\n",
    "\n",
    "# random forest\n",
    "para_dict[\"rf_use\"]=True\n",
    "para_dict[\"rf_nestimators\"]=10\n",
    "\n",
    "# svm classifiers with different kernels\n",
    "para_dict[\"linsvc_use\"]=True\n",
    "para_dict[\"rbfsvc_use\"]=True\n",
    "para_dict[\"rbfsvc_gamma\"]=\"auto\"\n",
    "para_dict[\"rbfsvc_probability\"]=True\n",
    "para_dict[\"sigmoidsvc_use\"]=True\n",
    "para_dict[\"sigmoidsvc_gamma\"]=\"auto\"\n",
    "para_dict[\"sigmoidsvc_probability\"]=True\n",
    "para_dict[\"polysvc_use\"]=True\n",
    "para_dict[\"polysvc_gamma\"]=\"auto\"\n",
    "para_dict[\"polysvc_probability\"]=True\n",
    "\n",
    "# gradient boosting\n",
    "para_dict[\"gradboost_use\"]=True\n",
    "para_dict[\"gradboost_maxdepth\"]=4\n",
    "para_dict[\"gradboost_learnrate\"]=0.05\n",
    "para_dict[\"gradboost_nestimators\"]=500\n",
    "para_dict[\"gradboost_minsamplessplit\"]=20\n",
    "para_dict[\"gradboost_maxfeatures\"]=0.2\n",
    "\n",
    "# mlp\n",
    "para_dict[\"mlp_use\"]=True\n",
    "para_dict[\"mlp_layers\"]=(100, 100)\n",
    "para_dict[\"mlp_maxiter\"]=5000\n",
    "para_dict[\"mlp_learnrate\"]=\"adaptive\"\n",
    "para_dict[\"mlp_batchsize\"]=32\n",
    "\n",
    "# knn classifiers with different distance metrics\n",
    "para_dict[\"uniformknn_use\"]=True\n",
    "para_dict[\"uniformknn_nneighbours\"]=15\n",
    "para_dict[\"distknn_use\"]=True\n",
    "para_dict[\"distknn_nneighbours\"]=15\n",
    "\n",
    "# ada boost\n",
    "para_dict[\"adaboost_use\"]=True\n",
    "#Quadratic Discriminant Analysis\n",
    "para_dict[\"quadDiscAna_use\"]=True\n",
    "\n",
    "# final estimator that combines the results of the ensamble\n",
    "para_dict[\"finalmlp_layers\"]=(50, 50)\n",
    "para_dict[\"finalmlp_maxiter\"]=5000\n",
    "para_dict[\"finalmlp_learnrate\"]=\"adaptive\"\n",
    "para_dict[\"finalmlp_batchsize\"]=32\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_stacked(X_train, y_train, X_val, y_val, para_dict, show_validation_perf=False, verbose=0):\n",
    "    # TODO: take different train/test splits to evaluate the performance development, #if show_validation_perf:\n",
    "    \n",
    "    # check if for every estimator in the ensamble it is defined to be used or not\n",
    "    para_dict_keys = para_dict.keys()\n",
    "    assert \"rf_use\" in para_dict_keys\n",
    "    assert \"linsvc_use\" in para_dict_keys\n",
    "    assert \"rbfsvc_use\" in para_dict_keys\n",
    "    assert \"sigmoidsvc_use\" in para_dict_keys\n",
    "    assert \"polysvc_use\" in para_dict_keys\n",
    "    assert \"gradboost_use\" in para_dict_keys\n",
    "    assert \"mlp_use\" in para_dict_keys    \n",
    "    assert \"uniformknn_use\" in para_dict_keys\n",
    "    assert \"distknn_use\" in para_dict_keys\n",
    "    assert \"adaboost_use\" in para_dict_keys\n",
    "    assert \"quadDiscAna_use\" in para_dict_keys\n",
    "\n",
    "    # create the estimators list dependent on para_dict:\n",
    "    estimators = []\n",
    "    if para_dict[\"rf_use\"]:\n",
    "        estimators.append(('rf', RandomForestClassifier(n_estimators=para_dict[\"rf_nestimators\"], random_state=para_dict[\"random_state\"])))\n",
    "    if para_dict[\"linsvc_use\"]:\n",
    "        estimators.append(('linsvc', LinearSVC(random_state=para_dict[\"random_state\"])))\n",
    "    if para_dict[\"rbfsvc_use\"]:\n",
    "        estimators.append(('rbfsvc', SVC(kernel = 'rbf', gamma=para_dict[\"rbfsvc_gamma\"], probability=para_dict[\"rbfsvc_probability\"], random_state=para_dict[\"random_state\"])))\n",
    "    if para_dict[\"sigmoidsvc_use\"]:\n",
    "        estimators.append(('sigmoidsvc', SVC(kernel = 'sigmoid', gamma=para_dict[\"sigmoidsvc_gamma\"], probability=para_dict[\"sigmoidsvc_probability\"], random_state=para_dict[\"random_state\"])))\n",
    "    if para_dict[\"polysvc_use\"]:\n",
    "        estimators.append(('polysvc', SVC(kernel = 'poly', gamma=para_dict[\"polysvc_gamma\"], probability=para_dict[\"polysvc_probability\"], random_state=para_dict[\"random_state\"])))\n",
    "    if para_dict[\"gradboost_use\"]:\n",
    "        estimators.append(('gradBoost', GradientBoostingClassifier( max_depth = para_dict[\"gradboost_maxdepth\"], learning_rate = para_dict[\"gradboost_learnrate\"], n_estimators = para_dict[\"gradboost_nestimators\"], \n",
    "                                                 min_samples_split = para_dict[\"gradboost_minsamplessplit\"], max_features = para_dict[\"gradboost_maxfeatures\"], random_state = para_dict[\"random_state\"] )))\n",
    "    if para_dict[\"mlp_use\"]:\n",
    "        estimators.append(('mlp', MLPClassifier(hidden_layer_sizes=para_dict[\"mlp_layers\"], max_iter=para_dict[\"mlp_maxiter\"], learning_rate=para_dict[\"mlp_learnrate\"], batch_size=para_dict[\"mlp_batchsize\"],\n",
    "                              random_state=para_dict[\"random_state\"])))\n",
    "    if para_dict[\"uniformknn_use\"]:\n",
    "        estimators.append(('uniformknn', KNeighborsClassifier(weights='uniform', n_neighbors = para_dict[\"uniformknn_nneighbours\"])))\n",
    "    if para_dict[\"distknn_use\"]:\n",
    "        estimators.append(('distknn', KNeighborsClassifier(weights='distance', n_neighbors = para_dict[\"distknn_nneighbours\"])))\n",
    "    if para_dict[\"adaboost_use\"]:\n",
    "        estimators.append(('adaboost', AdaBoostClassifier(random_state=para_dict[\"random_state\"])))\n",
    "    if para_dict[\"quadDiscAna_use\"]:\n",
    "        estimators.append(('quadDiscAna', QuadraticDiscriminantAnalysis()))\n",
    "    \n",
    "    # maybe to not train with all the data... X_train = X_train[:100] for testing etc\n",
    "    X_train = X_train\n",
    "    y_train = y_train\n",
    "    \n",
    "    # normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_val_norm = scaler.transform(X_val)\n",
    "    \n",
    "    # combining the previously defined classifiers into one Stacked Clf\n",
    "    clf = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        #final_estimator=LogisticRegression(), # try mlp\n",
    "        final_estimator=MLPClassifier(hidden_layer_sizes=para_dict[\"finalmlp_layers\"], max_iter=para_dict[\"finalmlp_maxiter\"], learning_rate=para_dict[\"finalmlp_learnrate\"], \n",
    "                                      batch_size=para_dict[\"finalmlp_batchsize\"], random_state=para_dict[\"random_state\"]),\n",
    "        verbose = verbose\n",
    "    )\n",
    "\n",
    "    # fit the ensemble clf on the train data\n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "    #create a prediction function that deploys the trained stacked classifier\n",
    "    predict_funct = lambda X: clf.predict(X)\n",
    "    \n",
    "    # define a score\n",
    "    score = lambda y, y_hat : 1 - f1_score( y, y_hat, average = \"micro\" )\n",
    "    train_loss_timeseries = np.repeat([ score( y_train, clf.predict( X_train_norm ))], 2 )\n",
    "        \n",
    "    if X_val is not None:\n",
    "        val_loss_timeseries = np.repeat([ score( y_val, clf.predict( X_val_norm ))], 2 )\n",
    "        return train_loss_timeseries, val_loss_timeseries, predict_funct\n",
    "    else:\n",
    "        return train_loss_timeseries, predict_funct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
